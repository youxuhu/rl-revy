# 4 动态规划

在问题变得复杂时，联立求解方程会变得难以使用。所以可以使用动态规划法对价值函数进行评估。

## 4.1 动态规划和策略评估

### 4.1.1 动态规划法简介

强化学习通常涉及两项任务：策略评估和策略控制  
$V_{k+1}(s)=\sum_{a,s'}p(s'|s,a){r(s,a,s')+\gamma V_k(s')}$  
$V_{k+1}(s)和V_k(s)$是推测值。这个式子的特点是使用“下一个可能的状态的价值函数$V_k(s')$来更新当前状态的价值函数$V_{k+1}(s)$”，这种使用估计值来改经估计值的过程叫做自举法。

### 4.1.2 尝试迭代策略评估

在迭代时使用“覆盖法”收敛速度会更快
dp.py

```
V = {'L1':0, 'L2':0.0}
new_V = V.copy()

cnt = 0 #用于计数
while True:
    new_V['L1'] = 0.5 *(-1+0.9*V['L1']+0.5*(1+0.9*V['L2']))
    new_V['L2'] = 0.5*(0+0.9*V['L1']+0.5*(-1+0.9*V['L2']))

    #更行量的最大值
    delta = abs(new_V['L1']-V['L1'])
    delat = max(delta, abs(new_V['L2']-V['L2']))
    V = new_V.copy()

    cnt += 1

    if delta <1e-4:
        print(V)
        print(cnt)
        break
```

dp_inplace.py

```
V = {'L1':0, 'L2':0.0}
cnt = 0 #用于计数

while True:
    t = 0.5 * (-1+0.9*V['L1']+0.5*(1+0.9*V['L2']))
    delta = abs(t-V['L1'])
    V['L1'] = t

    t = 0.5*(0+0.9*V['L1']+0.5*(-1+0.9*V['L2']))
    delta = max(delta,abs(t-V['L2']))
    V['L2'] = t

    cnt +=1
    if delta<1e-4:
        print(V)
        print(cnt)
        break
```

## 4.2 解决更大的问题

现在建立一个3\*4的世界，详细见书pg88

### 4.2.1 GridWorld类的实现

common/gridworld.py

```
import numpy as np
import common.gridworld_render as render_helper


class GridWorld:
    def __init__(self):
        self.action_space = [0, 1, 2, 3]
        self.action_meaning = {
            0: "UP",
            1: "DOWN",
            2: "LEFT",
            3: "RIGHT",
        }

        self.reward_map = np.array(
            [[0, 0, 0, 1.0],
             [0, None, 0, -1.0],
             [0, 0, 0, 0]]
        )
        self.goal_state = (0, 3)
        self.wall_state = (1, 1)
        self.start_state = (2, 0)
        self.agent_state = self.start_state

    @property
    def height(self):
        return len(self.reward_map)

    @property
    def width(self):
        return len(self.reward_map[0])

    @property
    def shape(self):
        return self.reward_map.shape

    # 返回可以使用的动作列表
    def actions(self):
        return self.action_space
    # 返回所有状态的生成器
    def states(self):
        for h in range(self.height):
            for w in range(self.width):
                yield (h, w)

    # 表示状态迁移的方法
    def next_state(self, state, action):
        action_move_map = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        move = action_move_map[action]
        next_state = (state[0] + move[0], state[1] + move[1])
        ny, nx = next_state
        # 如果越界或者撞墙则停在原地
        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:
            next_state = state
        elif next_state == self.wall_state:
            next_state = state
        # 返回下一个状态
        return next_state

    # 奖励函数
    def reward(self, state, action, next_state):
        return self.reward_map[next_state]

    # 重置环境，回到初始状态
    def reset(self):
        self.agent_state = self.start_state
        return self.agent_state
    # 执行action，并将时间前进一部
    def step(self, action):
        state = self.agent_state
        next_state = self.next_state(state, action)
        reward = self.reward(state, action, next_state)
        done = (next_state == self.goal_state)

        self.agent_state = next_state
        return next_state, reward, done

    def render_v(self, v=None, policy=None, print_value=True):
        renderer = render_helper.Renderer(self.reward_map, self.goal_state,
                                          self.wall_state)
        renderer.render_v(v, policy, print_value)

    def render_q(self, q=None, print_value=True):
        renderer = render_helper.Renderer(self.reward_map, self.goal_state,
                                          self.wall_state)
        renderer.render_q(q, print_value)

```

### 4.2.2 迭代策略评估的实现

```
from common.gridworld import *
from collections import defaultdict


def eval_onestep(pi, V, env, gamma = 0.9):

    # 遍历环境中的所有状态
    for state in env.states():  # env.states()返回所有可能的状态

        # 处理目标状态（终止状态）
        if state == env.goal_state:  # 如果当前状态是目标状态
            V[state] = 0  # 目标状态的价值始终为0（没有未来收益）
            continue  # 跳过后续计算，直接处理下一个状态

        # 获取当前状态下各行动的选择概率
        action_probs = pi[state]  # 例如：{'left': 0.25, 'right': 0.25, 'up': 0.25, 'down': 0.25}

        # 初始化当前状态的新价值为0
        new_V = 0

        # 遍历所有可能的行动
        for action, action_prob in action_probs.items():  # action_prob是采取该行动的概率
            # 根据当前状态和选择的行动，确定下一个状态
            next_state = env.next_state(state, action)

            # 获取执行该行动后获得的即时奖励
            r = env.reward(state, action, next_state)

            # 贝尔曼期望方程的核心计算：
            # 累加：行动概率 × (即时奖励 + 折扣因子 × 下一个状态的价值)
            # 注意：这里使用旧的V[next_state]值进行同步更新
            new_V += action_prob * (r + gamma * V[next_state])

        # 更新当前状态的价值为新计算的值
        V[state] = new_V

    # 返回更新后的价值函数
    return V


def policy_eval(pi, V, env, gamma, threshold = 0.001):

    # 无限循环，直到价值函数收敛
    while True:
        # 保存旧的价值函数，用于比较变化
        old_V = V.copy()  # 使用copy()避免引用同一个字典对象

        # 执行一次迭代更新，得到新的价值函数
        V = eval_onestep(pi, V, env, gamma)

        # 初始化最大变化量为0
        delta = 0

        # 遍历所有状态，找出价值变化的最大值
        for state in V.keys():
            # 计算当前状态的价值变化绝对值
            t = abs(V[state] - old_V[state])

            # 更新最大变化量
            if delta < t:
                delta = t

        # 检查收敛条件：如果所有状态的价值变化都小于阈值
        if delta < threshold:
            break  # 停止迭代，已收敛

    # 返回收敛后的价值函数
    return V


if __name__ == "__main__":
    env = GridWorld()
    gamma = 0.9

    pi = defaultdict(lambda:{0:0.25, 1:0.25, 2:0.25, 3:0.25})
    V = defaultdict(lambda: 0.0)
    V = policy_eval(pi, V, env, gamma)
    env.render_v(V)
```

## 4.3 策略迭代法

我们的目标是得到最策略。贝尔曼方程的计算量过大，我们使用先买你的方法。

### 4.3.1 策略的改进

策略的贪婪化意味着：

<ul>
    <li>策略总是被改进</li>
    <li>如果策略没有被改进，那么他就是最优解</li>
</ul>

## 4.4 实施策略迭代法

求解3\*4网格世界的问题，目标是使用策略迭代法得到最优策略。

### 4.4.1 改进策略

<font color=orange>$\mu'(s)=argmax_a\sum_{s'}P(s'|s,a)\{r(s,a,s') + \gamma V_\mu(s')\}$</font>

这个式子可以简化为
<font color=orange>$\mu'(s)=argmax_a\{r(s, a, s') + \gamma V_\mu(s')\}$</font>

```python
# argmax函数：返回字典中值最大的键
def argmax(d):
    max_value = max(d.values())
    max_keys = 0
    for key, value in d.items():
        if value == max_value:
            max_keys = key
    return max_keys

# 使用argmax函数将价值函数贪婪化
def greedy_policy(V, env, gamma):
    pi = {}
    for state in env.states():
        action_values = {}
        for action in env.actions():
            next_state = env.next_state(state, action)
            r = env.reward(state, action, next_state)
            value = r + gamma * V[next_state]
            action_values[action] = value

        max_action = argmax(action_values)
        action_probs = {0: 0, 1: 0, 2: 0, 3: 0}
        action_probs[max_action] = 1
        pi[state] = action_probs
    return pi
```

### 4.4.2 策略迭代法的实现

策略迭代法的核心思想是：

1. 策略评估：计算当前策略的价值函数
2. 策略改进：基于价值函数生成更好的策略
3. 重复上述过程直到策略收敛

ch04/policy_iter.py

```python
def policy_iter(env, gamma, threshold=1e-3, is_render=False):
    # 初始化为均匀随机策略
    pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})
    V = defaultdict(lambda: 0)

    while True:
        # 策略评估：评估当前策略pi的价值函数V
        V = policy_eval(pi, V, env, gamma, threshold)
        # 策略改进：根据价值函数V生成新策略
        new_pi = greedy_policy(V, env, gamma)
        if is_render:
            env.render_v(V, pi)

        # 检查策略是否收敛
        if new_pi == pi:
            break
        pi = new_pi
    return pi
```

## 4.5 价值迭代法

### 4.5.1 贝尔曼最优方程的迭代

价值迭代法直接基于贝尔曼最优方程进行迭代更新：

<font color=skyblue>$V_{k+1}(s)=\max_a\sum_{s'}p(s'|s,a)\{r(s,a,s')+\gamma V_k(s')\}$</font>

简化后：
<font color=orange>$V_{k+1}(s)=\max_a\{r(s,a,s')+\gamma V_k(s')\}$</font>

价值迭代法的优势在于不需要显式地维护策略，只需要迭代更新价值函数，最后根据收敛的价值函数求出最优策略。

### 4.5.2 价值迭代法的实现

ch04/value_iter.py

```python
def value_iter_onestep(V, env, gamma):
    for state in env.states():
        if state == env.goal_state:
            V[state] = 0
            continue

        action_values = []
        for action in env.actions():
            next_state = env.next_state(state, action)
            r = env.reward(state, action, next_state)
            value = r + gamma * V[next_state]
            action_values.append(value)

        # 取所有动作价值的最大值
        V[state] = max(action_values)
    return V

def value_iter(V, env, gamma, threshold=1e-3, is_render=True):
    while True:
        if is_render:
            env.render_v(V)

        old_V = V.copy()
        V = value_iter_onestep(V, env, gamma)

        # 计算最大变化量
        delta = 0
        for state in V.keys():
            t = abs(V[state]-old_V[state])
            if delta < t:
                delta = t

        # 判断是否收敛
        if delta < threshold:
            break
    return V
```

使用价值迭代法求解后，通过贪婪化得到最优策略：

```python
V = value_iter(V, env, gamma)
pi = greedy_policy(V, env, gamma)
env.render_v(V, pi)
```

## 4.6 总结

本章介绍了动态规划法在强化学习中的应用：

1. **迭代策略评估**：通过迭代更新计算给定策略的价值函数，使用自举法提高效率。

2. **策略迭代法**：
   - 策略评估 + 策略改进的循环
   - 保证收敛到最优策略
   - 需要在每次迭代中完全评估策略

3. **价值迭代法**：
   - 直接基于贝尔曼最优方程迭代
   - 每次迭代只更新一次价值函数
   - 通常比策略迭代法更高效

4. **关键技术**：
   - 覆盖法（in-place）更新可以加快收敛
   - argmax函数用于策略的贪婪化
   - 使用阈值判断收敛

动态规划法的前提是已知环境的完整模型（状态转移概率和奖励函数），在实际应用中，如果模型未知，则需要使用后续章节的无模型方法。
