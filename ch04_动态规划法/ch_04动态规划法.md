# 4 动态规划
在问题变得复杂时，联立求解方程会变得难以使用。所以可以使用动态规划法对价值函数进行评估。
## 4.1 动态规划和策略评估
### 4.1.1 动态规划法简介
强化学习通常涉及两项任务：策略评估和策略控制            
$V_{k+1}(s)=\sum_{a,s'}p(s'|s,a){r(s,a,s')+\gamma V_k(s')}$                   
$V_{k+1}(s)和V_k(s)$是推测值。这个式子的特点是使用“下一个可能的状态的价值函数$V_k(s')$来更新当前状态的价值函数$V_{k+1}(s)$”，这种使用估计值来改经估计值的过程叫做自举法。
### 4.1.2 尝试迭代策略评估
在迭代时使用“覆盖法”收敛速度会更快
dp.py
```
V = {'L1':0, 'L2':0.0}
new_V = V.copy()

cnt = 0 #用于计数
while True:
    new_V['L1'] = 0.5 *(-1+0.9*V['L1']+0.5*(1+0.9*V['L2']))
    new_V['L2'] = 0.5*(0+0.9*V['L1']+0.5*(-1+0.9*V['L2']))

    #更行量的最大值
    delta = abs(new_V['L1']-V['L1'])
    delat = max(delta, abs(new_V['L2']-V['L2']))
    V = new_V.copy()

    cnt += 1

    if delta <1e-4:
        print(V)
        print(cnt)
        break
```

dp_inplace.py
```
V = {'L1':0, 'L2':0.0}
cnt = 0 #用于计数

while True:
    t = 0.5 * (-1+0.9*V['L1']+0.5*(1+0.9*V['L2']))
    delta = abs(t-V['L1'])
    V['L1'] = t

    t = 0.5*(0+0.9*V['L1']+0.5*(-1+0.9*V['L2']))
    delta = max(delta,abs(t-V['L2']))
    V['L2'] = t

    cnt +=1
    if delta<1e-4:
        print(V)
        print(cnt)
        break
```

## 4.2 解决更大的问题
现在建立一个3*4的世界，详细见书pg88
### 4.2.1 GridWorld类的实现
common/gridworld.py
```
```