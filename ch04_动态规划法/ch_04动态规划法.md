# 4 动态规划
在问题变得复杂时，联立求解方程会变得难以使用。所以可以使用动态规划法对价值函数进行评估。
## 4.1 动态规划和策略评估
### 4.1.1 动态规划法简介
强化学习通常涉及两项任务：策略评估和策略控制            
$V_{k+1}(s)=\sum_{a,s'}p(s'|s,a){r(s,a,s')+\gamma V_k(s')}$                   
$V_{k+1}(s)和V_k(s)$是推测值。这个式子的特点是使用“下一个可能的状态的价值函数$V_k(s')$来更新当前状态的价值函数$V_{k+1}(s)$”，这种使用估计值来改经估计值的过程叫做自举法。
### 4.1.2 尝试迭代策略评估
在迭代时使用“覆盖法”收敛速度会更快
dp.py
```
V = {'L1':0, 'L2':0.0}
new_V = V.copy()

cnt = 0 #用于计数
while True:
    new_V['L1'] = 0.5 *(-1+0.9*V['L1']+0.5*(1+0.9*V['L2']))
    new_V['L2'] = 0.5*(0+0.9*V['L1']+0.5*(-1+0.9*V['L2']))

    #更行量的最大值
    delta = abs(new_V['L1']-V['L1'])
    delat = max(delta, abs(new_V['L2']-V['L2']))
    V = new_V.copy()

    cnt += 1

    if delta <1e-4:
        print(V)
        print(cnt)
        break
```

dp_inplace.py
```
V = {'L1':0, 'L2':0.0}
cnt = 0 #用于计数

while True:
    t = 0.5 * (-1+0.9*V['L1']+0.5*(1+0.9*V['L2']))
    delta = abs(t-V['L1'])
    V['L1'] = t

    t = 0.5*(0+0.9*V['L1']+0.5*(-1+0.9*V['L2']))
    delta = max(delta,abs(t-V['L2']))
    V['L2'] = t

    cnt +=1
    if delta<1e-4:
        print(V)
        print(cnt)
        break
```

## 4.2 解决更大的问题
现在建立一个3*4的世界，详细见书pg88
### 4.2.1 GridWorld类的实现
common/gridworld.py
```
import numpy as np
import common.gridworld_render as render_helper


class GridWorld:
    def __init__(self):
        self.action_space = [0, 1, 2, 3]
        self.action_meaning = {
            0: "UP",
            1: "DOWN",
            2: "LEFT",
            3: "RIGHT",
        }

        self.reward_map = np.array(
            [[0, 0, 0, 1.0],
             [0, None, 0, -1.0],
             [0, 0, 0, 0]]
        )
        self.goal_state = (0, 3)
        self.wall_state = (1, 1)
        self.start_state = (2, 0)
        self.agent_state = self.start_state

    @property
    def height(self):
        return len(self.reward_map)

    @property
    def width(self):
        return len(self.reward_map[0])

    @property
    def shape(self):
        return self.reward_map.shape

    # 返回可以使用的动作列表
    def actions(self):
        return self.action_space
    # 返回所有状态的生成器
    def states(self):
        for h in range(self.height):
            for w in range(self.width):
                yield (h, w)

    # 表示状态迁移的方法
    def next_state(self, state, action):
        action_move_map = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        move = action_move_map[action]
        next_state = (state[0] + move[0], state[1] + move[1])
        ny, nx = next_state
        # 如果越界或者撞墙则停在原地
        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:
            next_state = state
        elif next_state == self.wall_state:
            next_state = state
        # 返回下一个状态
        return next_state

    # 奖励函数
    def reward(self, state, action, next_state):
        return self.reward_map[next_state]

    # 重置环境，回到初始状态
    def reset(self):
        self.agent_state = self.start_state
        return self.agent_state
    # 执行action，并将时间前进一部
    def step(self, action):
        state = self.agent_state
        next_state = self.next_state(state, action)
        reward = self.reward(state, action, next_state)
        done = (next_state == self.goal_state)

        self.agent_state = next_state
        return next_state, reward, done

    def render_v(self, v=None, policy=None, print_value=True):
        renderer = render_helper.Renderer(self.reward_map, self.goal_state,
                                          self.wall_state)
        renderer.render_v(v, policy, print_value)

    def render_q(self, q=None, print_value=True):
        renderer = render_helper.Renderer(self.reward_map, self.goal_state,
                                          self.wall_state)
        renderer.render_q(q, print_value)

```

### 4.2.2 迭代策略评估的实现
```
from common.gridworld import *
from collections import defaultdict


def eval_onestep(pi, V, env, gamma = 0.9):
    
    # 遍历环境中的所有状态
    for state in env.states():  # env.states()返回所有可能的状态
        
        # 处理目标状态（终止状态）
        if state == env.goal_state:  # 如果当前状态是目标状态
            V[state] = 0  # 目标状态的价值始终为0（没有未来收益）
            continue  # 跳过后续计算，直接处理下一个状态
        
        # 获取当前状态下各行动的选择概率
        action_probs = pi[state]  # 例如：{'left': 0.25, 'right': 0.25, 'up': 0.25, 'down': 0.25}
        
        # 初始化当前状态的新价值为0
        new_V = 0

        # 遍历所有可能的行动
        for action, action_prob in action_probs.items():  # action_prob是采取该行动的概率
            # 根据当前状态和选择的行动，确定下一个状态
            next_state = env.next_state(state, action)
            
            # 获取执行该行动后获得的即时奖励
            r = env.reward(state, action, next_state)

            # 贝尔曼期望方程的核心计算：
            # 累加：行动概率 × (即时奖励 + 折扣因子 × 下一个状态的价值)
            # 注意：这里使用旧的V[next_state]值进行同步更新
            new_V += action_prob * (r + gamma * V[next_state])
        
        # 更新当前状态的价值为新计算的值
        V[state] = new_V
    
    # 返回更新后的价值函数
    return V


def policy_eval(pi, V, env, gamma, threshold = 0.001):
    
    # 无限循环，直到价值函数收敛
    while True:
        # 保存旧的价值函数，用于比较变化
        old_V = V.copy()  # 使用copy()避免引用同一个字典对象
        
        # 执行一次迭代更新，得到新的价值函数
        V = eval_onestep(pi, V, env, gamma)
        
        # 初始化最大变化量为0
        delta = 0
        
        # 遍历所有状态，找出价值变化的最大值
        for state in V.keys():
            # 计算当前状态的价值变化绝对值
            t = abs(V[state] - old_V[state])
            
            # 更新最大变化量
            if delta < t:
                delta = t
        
        # 检查收敛条件：如果所有状态的价值变化都小于阈值
        if delta < threshold:
            break  # 停止迭代，已收敛
    
    # 返回收敛后的价值函数
    return V


if __name__ == "__main__":
    env = GridWorld()
    gamma = 0.9

    pi = defaultdict(lambda:{0:0.25, 1:0.25, 2:0.25, 3:0.25})
    V = defaultdict(lambda: 0.0)
    V = policy_eval(pi, V, env, gamma)
    env.render_v(V)
```

## 4.3 策略迭代法
我们的目标是得到最策略。贝尔曼方程的计算量过大，我们使用先买你的方法。
### 4.3.1 策略的改进

策略的贪婪化意味着：
<ul>
    <li>策略总是被改进</li>
    <li>如果策略没有被改进，那么他就是最优解</li>
</ul>

## 4.4  实施策略迭代法
求解3*4网格世界的问题，目标是使用策略迭代法得到最优策略。

### 4.4.1 改进策略
$\mu'(s)=argmax_a\sum_{s'}P(s'|s,a)\{r(s,a,s') + \gamma V_\mu(s')\}$
这个式子可以简化为
$\mu'(s)=argmax_a\{r(s, a, s') + \gamma V_\mu(s')\}$
