# 2. 马尔可夫决策过程(MDP)

## 2.1 什么是MDP
决策过程是智能代理通过与环境互动决定其行动的过程
### 2.1.1 MDP的具体例子
智能代所处的情况根据其行动而发生改变。在强化学习中，这种情况被称为状态(state)。状态的变化取决于智能代理的行动， 智能代理在状态迁移后执行新的行动。

MDP中我们需要时间的概念，单位为“时间步”，实际单位取决于问题。

智能代理要考虑的是将来获得的奖励总和，而不是眼前的奖励。目标是奖励综合的最大化。

### 2.1.2 智能代理与环境的互动

在MDP中，智能代理与环境进行互动，智能代理根据当前状态选择一个行动(action)，环境根据智能代理的行动返回一个奖励(reward)和一个新的状态(new state)。智能代理的目标是最大化未来奖励的总和。

## 2.2 环境和智能代理的数学表示
### 2.2.1 状态迁移
### 2.2.2 奖励函数
### 2.2.3 智能代理的策略
策略的关键在于它使得智能代理仅根据当前状态来决定其行动，因为环境的迁移是符合马尔可夫性的。

环境迁移，奖励只和当前的状态和之后的状态相关。

## 2.3 MDP的目标
MDP的目标是找到最优策略，得到收益的最大化。
### 2.3.1 回合制任务和连续性任务
回合制任务有终点， 连续性任务没有终点。
# 2.3.2 收益

收益被定义为只能代理获得的奖励之和。例如：
$G_t=R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+...$
在这个例子中随着时间的推移，奖励会被$\gamma$指数级削弱。$\gamma$被称为折现率， 被设定为0.0-1.0之间的数。引入折现率是为了防止收益发散到无穷大。<font color=orange>折现率也使得近期奖励变得更加重要！！！</font>
### 2.3.3 状态价值函数
智能代理的目标是使这种这种收益最大化。智能代理可能随机决定行动，状态也可能随机迁移，在这种情况下获得的收益也将呈随机的特点。这意味着即使从相同的状态开始，不同回合的收益也随机变化。

为了处理这种随机行为，需要使用期望值或"收益的期望值"作为衡量标准。收益的期望值的数学式如下所示：
<font color=orange>$v_\pi(s)=E[G_t|S_t=s,\pi]$</font>

条件是状态$S_t$是s，智能代理的策略是$\pi$，我们使用$v_\pi (s)$来表示收益的期望值。
也可写为：<font color=orange>$v_\pi(s)=E_\pi[G_t|S_t=s]$</font>

### 2.3.4 最优策略和最优价值函数
在所有的状态下$v_{\pi'}(s)\geq v_\pi(s)$，当满足这个条件的时候可以说$\pi'$是比$\pi$好的策略，如果两个恒等可以说他们两个是一样好的策略。

如果用$\pi_*$来表示最优策略，那么则略$\pi_*$就是与其他策略相比，在所有的状态下的状态价值函数$v_{\pi_*}(s)$的值最大的策略。
<font color=orange>最优策略是确定性策略，每一个状态下的行动是唯一确定的！！</font>

# 2.4 MDP的例子