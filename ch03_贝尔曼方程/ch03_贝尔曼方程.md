# 3 贝尔曼方程

## 3.1 贝尔曼方程的推导

### 3.1.1 概率和期望值
x、y 同时发生的概率（联合概率）为：

$$
P(x, y)=P(x)P(y|x)
$$

奖励的期望值可以表示为下面的形式：

$$
E[r(x, y)] = \sum_x\sum_y p(x, y)r(x, y)
$$

### 3.1.2 贝尔曼方程的推导

$$
G_t = R_t + \lambda R_{t+1} + \lambda^2 R_{t+2} + ...
$$

$$
G_{t+1} = R_{t+1} + \lambda R_{t+2} + \lambda^2 R_{t+3} + ...
$$

由两式可以得到：

$$
G_t = R_t + \lambda G_{t+1}
$$

因为：

$$
v_\pi(s) = E_\pi[G_t|S_t=s]
$$

所以带入可得：

$$
E_\pi[G_t|S_t=s] = E_\pi[R_t + \lambda G_{t+1}|S_t=s]
$$

贝尔曼方程需要在书上在复习！！！

## 3.2 贝尔曼方程的应用

### 3.2.1 有两个方格的网格世界

### 3.2.2 贝尔曼方程的意义

贝尔曼方程能够将无限的计算转化为有限的联立方程。

## 3.3 行动价值函数与贝尔曼方程

### 3.3.1 行动价值函数

$$
q_\pi(s, a)=E_\pi[G_t|S_t=s, A_t=a]
$$

Q 函数的定义是：在状态 $s$ 下采取行动 $a$，然后按照策略 $\pi$ 进行行动，所能获得的期望回报。

可以得到：

$$
v_\pi(s)=\sum_a \pi(a|s)q_\pi(s,a)
$$

### 3.3.2 行动价值函数的贝尔曼方程

$$
q_\pi(s,a)=E_\pi[G_t|S_t=s, A_t=a]=E_\pi[R_t+\lambda G_{t+1}|S_t=s, A_t=a]
$$

最终得到：

$$
q_\pi(s, a)=\sum_{s'}p(s'|s,a)\{r(s,a,s')+\lambda\sum_{a'}\pi(a'|s')q_\pi(s',a')\}
$$

## 3.4 贝尔曼最优方程

### 3.4.1 状态价值函数的贝尔曼最优方程

$$
v_*(s)=\max_a\sum_{s'}p(s'|s,a)\{r(s,a,s')+\lambda v_*(s')\}
$$

### 3.4.2 Q函数的贝尔曼最优方程

$$
q_*(s,a)=\sum_{s'}p(s'|s,a)\{r(s,a,s')+\lambda \max_{a'}q_*(s',a')\}
$$

## 3.5 贝尔曼最优方程的实例

### 3.5.1 应用贝尔曼方最优解方程

当 $s'=f(s,a)$ 时：

$$
q_*(s,a)=\max_a\{r(s,a,s')+\lambda v_*(s')\}
$$

### 3.5.2 得到最优策略

$$
\mu_*=\arg\max_a\sum_{s'}p(s'|s,a)\{r(s,a,s')+\lambda v_*(s')\}
$$

由此可知，一旦知道了最优状态价值函数，就可以得到最优策略。
