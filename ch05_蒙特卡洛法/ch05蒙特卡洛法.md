# 5 蒙特卡洛方法

## 5.1 蒙特卡洛方法的基础知识
### 5.1.1 骰子的点数之和
两个骰子点数之和
```python
ps = {2:1/36, 3:2/36, 4:3/36, 5:4/36, 6:5/36, 7:6/36, 8:5/36, 9:4/36, 10:3/36, 11:2/36, 12:1/36}
V = 0
for x, p in ps.items():
    V += x*p
print(V)
```
上面的代码根据输入的概率分布求期望值。只要知道概率分布就可以向上面那样计算期望值。

### 5.1.2 分布模型和样本模型
将一个表示概率分布的模型称为<strong>分布模型</strong>.
样本模型是只需要采样的模型。分布模型和样本模型的区别在于，分布模型需要知道概率分布，而样本模型只需要采样。

在样本模型中不需要显示的提供概率分布，唯一的要求就是可以进行采样。
下面来实际的实现采样模型。本次使用的是样本模型。
```python
import numpy as np

def sample(dices=2):
    x = 0
    for _ in range(dices):
        x += np.random.choice([1, 2, 3, 4, 5, 6]) # 以相同的概率在例表选取一个
    return x

if __name__ == "__main__":
    for _ in range(10):
        print(sample())
```
### 5.1.3 蒙特卡洛方法的实现
下面编写一个使用蒙特卡洛法求期望值的代码
```python
if __name__ == "__main__":
    trial = 1000
    samples = []
    for _ in range(trial):
        s = sample()
        samples.append(s)

    V = sum(samples) / len(samples)
    print(f"V = {V}")
```

使用增量法来计算，计算会更加的高效
```python
if __name__ == "__main__":
    trial = 1000
    V, n = 0, 0
    for _ in range(trial):
        s = sample()
        n += 1
        V += (s-V)/n
        print(f"Trial: {n}, Sample: {s}, Average: {V:.2f}")
```
上面的代码在运行的过程中，会在每次获得样本数据时计算平均值。结果显示，随着样本数据的增加，平均值逐渐接近正确答案7。

## 5.2 使用蒙特卡洛方法评估策略

我们可以用智能代理在实际行动中获得的经验来估计价值函数。
### 5.2.1 使用蒙特卡洛方法计算价值函数

蒙特卡洛方法只能使用在回合制任务中。由于连续性任务没有“结束”，因此收益的样本数据无法确定。

<font color=orange>具体计算方式见书p126</font>


### 5.2.2 求所有状态的价值函数
在下面的情况中
A->B->C->end
<font color = orange>$G_A=R_0+\gamma R_1+\gamma^2 R_2$</font>
在这个过程中经过了B
<font color = orange>$G_B=R_1+\gamma R_2$</font>
同理我们可以得到C的收益
<font color = orange>$G_C = R_2$</font>

通过这样的计算我们在一次实验中就可以得到3个状态的收益。

### 5.2.3 蒙特卡洛方法的高效实现

<font color = orange>$
G_A = R_0 + \gamma R_1 + \gamma^2 R_2$
$G_B = R1 + \gamma R_2$
$G_C = R_2$
</font>
这些式子可以优化为


$
G_A = R_0 + \gamma G_B
G_B = R_1 + \gamma G_C
G_C = R_2
$

## 5.3 蒙特卡罗方法的实现
### 5.3.1 step方法
GridWorld中有一个step方法，可以用来使智能代理采取行动。
```python
from common.gridworld import GridWorld

env = GridWorld()
action = 0
next_state, reward, done = env.step(action)
```

### 5.3.2 智能代理类的实现

智能代理类的实现如下所示

```python
from collections import defaultdict
import numpy as np
from common.gridworld import GridWorld

class RandomAgent:
    def __init__(self):
        self.gamma = 0.9
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions) # 策略
        self.V = defaultdict(lambda: 0)# 价值函数
        self.cnts = defaultdict(lambda: 0)# 用于通过增量式实现收益的平均处理值上
        self.memory = []# 持续记录状态、动作、奖励的列表

    def get_action(self, state):
        action_prob = self.pi[state]
        actions = list(action_prob.keys())
        probs = list(action_prob.values())
        return np.random.choice(actions, p = probs)
    
    def add(self, state, action, reward):
        data = (state, action, reward)
        self.memory.append(data)

    def reset(self):
        self.memory.clear()
    
    def eval(self):
        G = 0
        for data in reversed(self.memory):
            state, action , reward = data
            G = self.gamma *G + reward
            self.cnts[state] += 1
            self.V[state] += (G-self.V[state])/self.cnts[state]


if __name__ == "__main__":
    env = GridWorld()
    agent = RandomAgent()
    episodes = 1000
    
    for episode in range(episodes):
        state = env.reset()
        agent.reset()

        while True:
            action = agent.get_action(state)
            next_state, reward, done = env.step(action)

            agent.add(state, action, reward)

            if done:
                agent.eval()
                break

            state = next_state
    env.render_v(agent.V)
```

## 5.4 使用蒙特卡罗方法的策略控制

### 5.4.1 评估和改进
对状态价值函数的评估:
<font color=orange>
$V_n(s)=\frac{G^{(1)}+G^{(2)}+\cdots+G^{(n)}}{n}$
$V_n(s)=V_{n-1}(s)+ \frac{G{(n)}-V_{n-1}(s)}{n}$
</font>
对Q函数的评估:
<font color=orange>
$Q_n(s, a)=\frac{G^{(1)}+G^{(2)}+\cdots+G^{(n)}}{n}$
$Q_n(s, a)=Q_{n-1}(s, a)+ \frac{G{(n)}-Q_{n-1}(s, a)}{n}$ 
</font>
不管是状态价值函数的估值还是行动价值函数的估值，在蒙特卡洛方法中的计算方式都是相同的，仅仅是对象发生了改变。

### 5.4.2 使用蒙特卡罗方法实现策略控制
我们使用蒙特卡洛方法实现策略控制的智能代理。将其实现为McAgent类。
```python
class McAgent:
    def __init__(self):
        self.gamma = 0.9
        self.action_size = 4

        random_actions = {0:0.25, 1:0.25, 2:0.25, 3:0.25}
        self.pi = defaultdict(lambda:random_dict)
        self.Q = defaultdict(lambda: 0) #使用Q不使用V
        self.cnts =defaultdict(lambda: 0)
        self.memory = []
    
    def get_action(self, state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        porbs = list(action_porbs.values())
        return np.random.choice(actions, p = probs)
    
    def add(self, state, action,reward):
        data = (state, action, reward)
        self.memory.append(data)
    
    def reset(self):
        self.memory.clear()
```
上面的代码和RandomAgent类几乎完全相同，唯一的区别是self.V被替换成了self.Q。
接下来我们实现策略控制
```python
# 外部函数，不在智能代理中，可以在后面重复使用
def greedy_probs(Q, state, action_size = 4):
    qs = [Q[(state, action)] for action in range(actionsize)] 
    max_action = np.argmax(qs)

    action_probs = {action:0.0 for action in range(action_size)}

    action_probs[max_action] = 1
    return action_probs
class McAgent:
    ...
    def update(self):
        G = 0
        for data in reversed(self.memory):
            state, action, reward = data
            G = self.gamma * G + reward
            key = (state, action)
            self.cnts[key] += 1
            self.Q[key] += (G-self.Q[key])/self.cnts[key]

            self.pi[state] = greedy_probs(self.Q, state)

```

### 5.4.3 epsilon-greedy策略
如果只进行贪婪处理，智能代理的路径将会被固定为一条。这样我们就无法收集所有状态和行动组合收益的样本数据了。为了解决这个问题，需要让智能代理进行探索。
让智能代理进行探索的一种方式是$\epsilon-greedy$，在智能代理的行动中增加一点随机性。


```
def greedy_probs(Q, state, epsilon=0, action_size=4):
    qs = [Q[(state, action)] for action in range(action_size)]
    max_action = np.argmax(qs)

    base_prob = epsilon / action_size
    action_probs = {action: base_prob for action in range(action_size)}
    #此时的action_probs是一个字典，键是动作，值是每个动作被选中的概率
    action_probs[max_action] += 1-epsilon
    return action_probs
```

### 5.4.4 修改为固定$\alpha$的方式


## 5.5 异策略型和重要性采样
#### 5.5.1 同策略型和异策略型

